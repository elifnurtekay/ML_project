# -*- coding: utf-8 -*-
"""proje.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xv5W54E38nOX0ZoH-6_dp37vQ8iHoROm

Importing Required Libraries
"""

from sklearn.linear_model import LinearRegression
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from numpy import sqrt

"""Gathering and Observing Data"""

df = pd.read_csv("Melbourne_housing_FULL.csv")
df_copy = df.copy()
df.head()

df_copy.dtypes

df_copy.columns

"""Exploratory Data Analysis"""

df_copy.info()

print(f'Shape     : {df_copy.shape}\n'
      f'Size      : {df_copy.size}\n'
      f'Dimension : {df_copy.ndim}')

numeric_columns = df_copy.select_dtypes(include=['number']).columns
categorical_columns=df_copy.select_dtypes(include=['object']).columns

for col in numeric_columns:
  z_scores=(df_copy[col] - df_copy[col].mean())/df_copy[col].std()
  outliers=(z_scores < -3) & (z_scores > 3)
  df_copy[col][outliers] = np.nan

df_copy

categorical_columns

numeric_columns

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df_copy1 = df.copy()

for i in categorical_columns:
  df_copy1[i]=label_encoder.fit_transform(df_copy1[i])
df_copy1

df_copy1.isnull().sum()

sns.heatmap(df_copy1.isnull(),cbar=False)

df_copy1.drop(columns=["Postcode", "Landsize", "BuildingArea", "Lattitude", "Longtitude", "Propertycount"], inplace=True)
YearBuilt_mode=df_copy1["YearBuilt"].fillna(df_copy1["YearBuilt"].mode().iloc[0])
df_copy1["YearBuilt"].fillna(YearBuilt_mode, inplace=True)
Bedroom2_mode=df_copy1["Bedroom2"].fillna(df_copy1["Bedroom2"].mode().iloc[0])
df_copy1["Bedroom2"].fillna(Bedroom2_mode, inplace=True)
Bathroom_mode=df_copy1["Bathroom"].fillna(df_copy1["Bathroom"].mode().iloc[0])
df_copy1["Bathroom"].fillna(Bathroom_mode, inplace=True)
distance_mode=df_copy1["Distance"].fillna(3060)
df_copy1["Distance"].fillna(distance_mode, inplace=True)

Price_mode=df_copy1["Price"].fillna(df_copy1["Price"].mode().iloc[0])
df_copy1["Price"].fillna(Price_mode, inplace=True)

Car_mode=df_copy1["Car"].fillna(df_copy1["Car"].mode().iloc[0])
df_copy1["Car"].fillna(Car_mode, inplace=True)

df_copy1.isnull().sum()

sns.heatmap(df_copy1.isnull(),cbar=False)

"""Data Visualization"""

sns.histplot(data=df_copy1["Price"], kde=True, color="blue")

# Creates histogram to the relationships between numerical columns with price
fig, axes = plt.subplots(2, 2, sharex=True, figsize=(12,4))
plt.subplots_adjust(top=0.80)
fig.suptitle('The relationship between all numerical variables and the Price variable', fontsize=16)
axes[0,0].set_title('Rooms with Price')
sns.histplot(ax=axes[0,0], data=df_copy1, x="Price", y="Rooms")

axes[0,1].set_title('Distance with Price')
sns.histplot(ax=axes[0,1], data=df_copy1, x="Price", y="Distance")

axes[1,0].set_title('Bathroom with Price')
sns.histplot(ax=axes[1,0], data=df_copy1, x="Price", y="Bathroom")

axes[1,1].set_title('Car with Price')
sns.histplot(ax=axes[1,1], data=df_copy1, x="Price", y="Car")
plt.savefig('relationship_with_price.png')

df_copy1.corr(numeric_only=True).T
df_copy1.corrwith(other=df_copy1["Price"], numeric_only=True)
# Correlation Matrix
sns.heatmap(df_copy1.corr(numeric_only=True).T,cmap="Blues", fmt=".1f",annot=True, linewidths=1,)

"""Model Selection

The input data (x,regressor)
"""

x = df_copy1.drop(columns='Price', axis=1)

x.shape

print(x)

"""The output data(y,predictor)"""

y = df_copy1['Price']

y.shape

print(y)

print(type(y))

"""splitting the dataset              """

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""Exploring the train and test datasets"""

print('Shape of X_train is {}'.format(x_train.shape))
print('Shape of X_test is {}'.format(x_test.shape))

print('Shape of y_train is {}'.format(y_train.shape))
print('Shape of y_test is {}'.format(y_test.shape))

from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet

from sklearn.model_selection import cross_validate
import sklearn.ensemble
from sklearn.ensemble import AdaBoostRegressor

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
def regression_models(x,y,predict_test):

    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

    L=LinearRegression()
    R=Ridge()
    Lass=Lasso()
    E=ElasticNet()
    ExTree=ExtraTreesRegressor()
    GBR=GradientBoostingRegressor()
    KN=KNeighborsRegressor()

    algos=[L,R,Lass,E,ExTree,GBR,KN]
    algo_names=['LinearRegression','Ridge','Lasso','ElasticNet', 'ExtraTreeRegressor','GradientBoostingRegressor','KNeighborsRegressor']
    r_squared=[]
    rmse=[]
    mae=[]
    mse=[]

    result=pd.DataFrame(columns=['R_Squared','RMSE','MAE','MSE'],index=algo_names)

    for item in algos:
        item.fit(x_train,y_train)
        item.predict(x_test)
        r_squared.append(r2_score(y_test,item.predict(x_test)))
        rmse.append((mean_squared_error(y_test,item.predict(x_test)))**.5)
        mae.append(mean_absolute_error(y_test,item.predict(x_test)))
        mse.append(mean_squared_error(y_test,item.predict(x_test)))

    result.R_Squared=r_squared
    result.RMSE=rmse
    result.MAE=mae
    result.MSE=mse

    return result.sort_values('R_Squared',ascending=False)

regression_models(x,y,predict_test=x_test)

"""1. R^2 (pronounced r-squared) or coefficient of determination
2. Mean absolute error(MAE)
3. Mean squared error(MSE)
**R^2**
what R-squared does:Compares your models predictions to the mean of the targets.Values can range from negative infinity(a very poor model) to 1. For example,if all your model does it predict the mean of the targets,it's R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1.

**R^2 (pronounced r-squared) or coefficient of determination**

**Mean absolue error(MAE)**                        
MAE is the average of the abostute differences between predictions and actual values. It gives you an idea of how wrong your models predictions are.

**Mean squared error (MSE)**

COMMENTS
"""